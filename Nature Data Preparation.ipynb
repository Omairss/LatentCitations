{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The notebook conisits for all steps needed to preprocess the Nature Dataset to perform frequent itemset mining of the citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import zipfile\n",
    "import os\n",
    "import xmltodict\n",
    "import json\n",
    "import zlib\n",
    "import seaborn as sns\n",
    "import resource\n",
    "import sys\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import nltk.data\n",
    "import time\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy.testing import assert_array_equal\n",
    "import threading\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "#nltk.download('punkt')\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_mapping(filename, content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given XML filename and XML file, extract rid mappings and attribute data\n",
    "    \"\"\"\n",
    "    \n",
    "    mappings  = {}\n",
    "    extracted = {}\n",
    "    references = {}\n",
    "    parsed    = xmltodict.parse(content.decode('UTF-8'))\n",
    "    soup      = BeautifulSoup(content)\n",
    "\n",
    "    if '@id' in parsed['article']:\n",
    "        extracted['id']       = str(parsed['article']['@id'])\n",
    "    if '@language' in parsed['article']:\n",
    "        extracted['language'] = str(parsed['article']['@language'])\n",
    "    if '@publish' in parsed['article']:\n",
    "        extracted['publish']  = str(parsed['article']['@publish'])\n",
    "    if '@relation' in parsed['article']:\n",
    "        extracted['relation'] = str(parsed['article']['@relation'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('jtl' in parsed['article']['pubfm']):\n",
    "                extracted['jtl']   = str(parsed['article']['pubfm']['jtl'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('vol' in parsed['article']['pubfm']):\n",
    "                extracted['vol']   = str(parsed['article']['pubfm']['vol'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('issue' in parsed['article']['pubfm']):\n",
    "                extracted['issue'] = str(parsed['article']['pubfm']['issue'])\n",
    "\n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('vol' in parsed['article']['pubfm']):\n",
    "                extracted['doi']   = str(parsed['article']['pubfm']['doi'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('fm' in parsed['article']):\n",
    "            if ('atl' in parsed['article']['fm']):\n",
    "                extracted['title']   = str(parsed['article']['fm']['atl'])\n",
    "     \n",
    "    \n",
    "    del parsed\n",
    "    \n",
    "    for bib in soup.find_all(\"bib\"):\n",
    "        \n",
    "        try:\n",
    "            reference_attr = {}\n",
    "            \n",
    "            reference_attr['title']   = str(bib.atl.contents[0])\n",
    "            reference_attr['snm']     = str([i.contents[0] for i in bib.find_all('snm')])\n",
    "            reference_attr['fnm']     = str([i.contents[0] for i in bib.find_all('fnm')])\n",
    "            reference_attr['journal'] = str(bib.jtl.contents[0])\n",
    "            reference_attr['year']    = str(bib.find_all('cd')[0].contents[0])\n",
    "\n",
    "            references.update({bib.attrs['id']: reference_attr})\n",
    "        \n",
    "        except Exception as e:\n",
    "            \n",
    "            if DEBUG == True:\n",
    "            \n",
    "                print('='*50)\n",
    "                print('Something is wrong with BeatifulSoup Tags: %s' % str(bib))\n",
    "                for i in ['snm', 'fnm', 'jtl', 'year', 'atl']:\n",
    "                    if len(bib.find_all(i)) == 0:\n",
    "                        print('%s attribute is missing.' %i)\n",
    "            \n",
    "            else: pass\n",
    "            \n",
    "    \n",
    "    references = {'metadata': extracted, 'references': references}\n",
    "    \n",
    "    try:\n",
    "        with open('data/intermediate/references/%s' % (filename + '.json'), 'w') as f:\n",
    "            f.write(json.dumps(references))\n",
    "        return True\n",
    "    \n",
    "    except TypeError as e:\n",
    "        #print('Some contents of the file %s is not serializable' % filename)\n",
    "        raise e \n",
    "        \n",
    "def get_reference_text(filename, content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given XML filename and XML file, extract referencing text and reference metadata\n",
    "    TODO: Compile all regex to make it faster\n",
    "    \n",
    "    Returns  {str(unique_paper_indentifier), list(preceeding_text)}\n",
    "    \"\"\"\n",
    "    \n",
    "    content   = content.decode('UTF-8')\n",
    "    bibid     = re.findall(r'<bibr\\srid=\\\"(.*?)\\\"\\s*\\/>', content)\n",
    "    #bibtext   = re.findall(r\"\\s.*?<bibr\\s\", content)\n",
    "    \n",
    "    bibtext_intermediate = [s for s in sent_detector.tokenize(content)\\\n",
    "                               if re.search(r'<bibr\\srid', s)]\n",
    "\n",
    "    bibtext_intermediate = [s.split('</p>') for s in bibtext_intermediate]\n",
    "    bibtext_intermediate = [item for sublist in bibtext_intermediate for item in sublist]\n",
    "\n",
    "    bibtext_intermediate = [s for s in bibtext_intermediate if re.search(r'<bibr\\srid', s)]\n",
    "    text_counter         = [len(re.findall(r'<bibr\\srid', s)) for s in bibtext_intermediate]\n",
    "\n",
    "    bibtext = []\n",
    "\n",
    "    for i, s in enumerate(bibtext_intermediate):\n",
    "        while text_counter[i] != 0:\n",
    "            bibtext.append(s)\n",
    "            text_counter[i] -= 1\n",
    "        \n",
    "    \n",
    "    assert(len(bibid) == len(bibtext)), \"The bibid's and preceeding text don't match: \" +\\\n",
    "                                        \"for article %s\\n\" % filename +\\\n",
    "                                        \"bibid: %s\\n\" %bibid +\\\n",
    "                                        \"bibtext: %s\\n\" %bibtext\n",
    "    \n",
    "    with open('data/intermediate/tex_ref_mappings/%s' % (filename + '.json'), 'w') as f:\n",
    "        f.write(json.dumps([{i[1]:i[0].split(' ')} for i in zip(bibid, bibtext)]))\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "def get_zips():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to extract relevant files from the filebase\n",
    "    \"\"\"\n",
    "    \n",
    "    citations = {}\n",
    "    directory = 'data/nature/raw xml/'\n",
    "    zip_list  = [i for i in os.listdir(directory) if 'supp_xml' not in i]\n",
    "    \n",
    "    errored   = []\n",
    "    #zfile     = zipfile.ZipFile(os.path.join(directory, zip_list[0]))\n",
    "    \n",
    "    for zfile in tqdm(zip_list):\n",
    "        \n",
    "        zfile     = zipfile.ZipFile(os.path.join(directory, zfile))\n",
    "\n",
    "        for finfo in zfile.infolist():\n",
    "\n",
    "            if 'nature' in finfo.filename:\n",
    "\n",
    "                try:\n",
    "                    ifile = zfile.open(finfo)\n",
    "                    content = ifile.read()\n",
    "                    tex_ref_map = get_reference_text(finfo.filename, content)\n",
    "                    ref_id_ref_map = get_reference_mapping(finfo.filename, content)\n",
    "\n",
    "                    citations[finfo.filename] = {'tex_ref_map': tex_ref_map,\n",
    "                                                'ref_id_ref_map': ref_id_ref_map}\n",
    "                except Exception as e:\n",
    "\n",
    "                    print('Filename: %s, Zip:%s, Content:%s' % (finfo.filename,\n",
    "                                                                zfile,\n",
    "                                                                content))\n",
    "                    errored.append({'Filename':finfo.filename,\n",
    "                                    'Zip':zfile ,\n",
    "                                    'Content': content})\n",
    "                    #return content\n",
    "                    #raise Exception\n",
    "            \n",
    "    return citations, errored\n",
    "        \n",
    "    \n",
    "def get_zips_parallel_mapper(directory):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to extract relevant files from the filebase\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        zip_list  = [i for i in os.listdir(directory) if 'supp_xml' not in i]\n",
    "        errored   = []\n",
    "        finfos    = []\n",
    "        contents  = []\n",
    "\n",
    "        for zfile in tqdm(zip_list):\n",
    "\n",
    "            zfile     = zipfile.ZipFile(os.path.join(directory, zfile))\n",
    "\n",
    "            for finfo in zfile.infolist():\n",
    "\n",
    "                if 'nature' in finfo.filename:\n",
    "                    \n",
    "                    ifile = zfile.open(finfo)\n",
    "                    content = ifile.read()\n",
    "\n",
    "                    finfos.append(finfo)\n",
    "                    contents.append(zlib.compress(content))\n",
    "\n",
    "        return finfos, contents\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print('Could not read file from zip %s, file %s' % (zfile, finfo.name))\n",
    "\n",
    "        \n",
    "        \n",
    "def get_zips_parallel_reducer(arg):\n",
    "    \n",
    "    try:\n",
    "        finfo, content = arg[0], zlib.decompress(arg[1])\n",
    "        del arg\n",
    "\n",
    "        #citations = {}\n",
    "        #tex_ref_map = get_reference_text(finfo.filename, content)\n",
    "        #ref_id_ref_map = get_reference_mapping(finfo.filename, content)\n",
    "        #citations[finfo.filename] = {'tex_ref_map': tex_ref_map,\n",
    "        #                            'ref_id_ref_map': ref_id_ref_map}\n",
    "        \n",
    "        get_reference_text(finfo.filename, content)\n",
    "        get_reference_mapping(finfo.filename, content)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        #print('Could not extract references from %s' % finfo)\n",
    "        return str(Exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "    Process and map texts in an article to thier references. T\n",
    "    \n",
    "    Input: The Nature Dataset of XML Files.\n",
    "    Output:\n",
    "        - tex_ref_mappings: This file consists of sentences associated with any given citation.\n",
    "        - references: A list of jsons with each item consisting of references and refereces details in the paper.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finfos, contents = get_zips_parallel_mapper(directory)\n",
    "p = Pool(6)\n",
    "success = p.map(get_zips_parallel_reducer, zip(finfos, contents))\n",
    "error_rate = sum([1 if i == True else 0 for i in success])/len(success)\n",
    "print ('%f of all processed files Succeeded' % error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: \n",
    "    \n",
    "    \n",
    "    \tGroups together all\n",
    "    \treferring sentences to the paper that they are referring to\n",
    "    \tin order to make the process easier for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ref_index(tex_ref_mappings, reference):\n",
    "\n",
    "    inverse_tex_ref = {}\n",
    "    \n",
    "    for tex_refs in tex_ref_mappings:\n",
    "        \n",
    "        text, refs = list(tex_refs.items())[0]\n",
    "        \n",
    "        for ref in refs: \n",
    "            if ref in inverse_tex_ref:\n",
    "                inverse_tex_ref[ref].append(text)\n",
    "            else:\n",
    "                inverse_tex_ref[ref] = [text]\n",
    "    \n",
    "    return inverse_tex_ref\n",
    "    \n",
    "def read_files():\n",
    "    \n",
    "    directory = 'data/intermediate/'\n",
    "    tex_ref_mappings = []\n",
    "    references       = []\n",
    "    \n",
    "    tex_ref_files = os.listdir(os.path.join(directory, 'tex_ref_mappings'))\n",
    "    references_files = os.listdir(os.path.join(directory, 'references'))\n",
    "    \n",
    "    if len(tex_ref_files) != len(references_files):\n",
    "        print('Not all text files have a references counterpart.\\n'\n",
    "              'Continuing with the files that do have a mapping')\n",
    "    \n",
    "    #file = tex_ref_files[1]\n",
    "    \n",
    "    for file in tqdm(tex_ref_files):\n",
    "        #print(file)\n",
    "        #print(tex_ref_mappings)\n",
    "        \n",
    "        with open(os.path.join(directory, 'tex_ref_mappings', file), 'r', encoding = 'UTF-8') as f0:\n",
    "\n",
    "            try:\n",
    "                with open(os.path.join(directory, 'references', file), 'r',  encoding = 'UTF-8') as f1:\n",
    "\n",
    "                    tex_ref_mappings.append(json.loads(str(f0.read()),  encoding = 'UTF-8'))\n",
    "                    references.append(json.loads(str(f1.read()),  encoding = 'UTF-8'))\n",
    "\n",
    "            except IOError as e:\n",
    "                print('References file %f not found.' % file)\n",
    "    \n",
    "    return tex_ref_mappings, references\n",
    "\n",
    "\n",
    "def get_reference_map_0(nature2references, nature_references_info, doi):\n",
    "    \n",
    "    try:\n",
    "        article_id = nature_references_info[nature_references_info.index == doi]['ArticleID'].values[0]\n",
    "    \n",
    "        references = nature2references[nature2references['CitingArticleID'] == article_id]\n",
    "    \n",
    "        references = pd.merge(references, nature_references_info,\n",
    "                              how = 'left', left_on = 'CitedArticleID', right_on = 'ArticleID')\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print('doi: %s' % doi)\n",
    "        print(e)\n",
    "        references = pd.DataFrame()\n",
    "    \n",
    "    return references\n",
    "\n",
    "def get_reference_map_1(get_reference_map_1_input): \n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: Improvements Todo: Force 1-1 mapping, add year and journal to increase map accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    reference, tex_ref_mapping = get_reference_map_1_input\n",
    "    \n",
    "    if 'doi' in reference['metadata']:\n",
    "        doi = reference['metadata']['doi']\n",
    "\n",
    "        references_0 = get_reference_map_0(nature2references, nature_references_info, doi).fillna('')\n",
    "\n",
    "        references_1 = pd.DataFrame.from_dict(reference['references'], orient = 'index').fillna('')\n",
    "\n",
    "        if (len(references_1) > 0) and (len(references_0) > 0):\n",
    "\n",
    "            references_1['text'] = references_1.index.map(build_ref_index(tex_ref_mapping, reference))\n",
    "\n",
    "            vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
    "            tfidf = vect.fit(list(references_0['Title']) + list(references_1['title']))\n",
    "\n",
    "            vect_0 = vect.transform(list(references_0['Title']))\n",
    "            vect_1 = vect.transform(list(references_1['title']))\n",
    "\n",
    "            pairwise_similarity = vect_0 * vect_1.T \n",
    "\n",
    "            references_0['match_index'] = [i[0] for i in np.argmax(pairwise_similarity, axis = 1).tolist()]\n",
    "\n",
    "            test_match = pd.merge(references_0,\n",
    "                                  references_1.reset_index(),\n",
    "                                  left_on = 'match_index',\n",
    "                                  right_index = True, how = 'left').rename(columns = {'Title': 'title0',\n",
    "                                                                                     'title': 'title1'})\n",
    "\n",
    "            return test_match\n",
    "\n",
    "    else:\n",
    "\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def flatten(l):\n",
    "    flat_list = []\n",
    "    for sublist in l:\n",
    "        if type(sublist) == list:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "    return flat_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_ref_mappings, references = read_files()\n",
    "\n",
    "nature_references_info = pd.read_hdf('data/nature/preprocessed/NatureReferencesInformation.hdf').set_index('DOI')\n",
    "nature2references      = pd.read_hdf('data/nature/preprocessed/Nature2References.hdf')\n",
    "\n",
    "mp_input = zip(references, tex_ref_mappings)\n",
    "p        = Pool(8)\n",
    "\n",
    "matched_dfs = p.map(get_reference_map_1, mp_input)\n",
    "matched_dfs = pd.concat(matched_dfs)\n",
    "matched_dfs = pd.DataFrame(matched_dfs.groupby('ArticleID')\\\n",
    "                                 .apply(lambda x: list(x['text'].values))).rename(columns = {0: 'text'})\n",
    "\n",
    "matched_dfs['clean_text'] = matched_dfs['text'].apply(flatten)\n",
    "matched_dfs['clean_text'] = matched_dfs['clean_text'].apply(lambda x: [BeautifulSoup(i, \"lxml\").text for i in x])\n",
    "matched_dfs['count'] = matched_dfs['clean_text'].apply(lambda x: len(x))\n",
    "matched_dfs = matched_dfs[matched_dfs['count'] != 0]\n",
    "matched_dfs.to_csv('data/intermediate/matched_dfs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2:\n",
    "    \n",
    "    Create a file of new line seperated jsons with each json consisting of all parsed \n",
    "\tinformation of a given paper. \n",
    "    \n",
    "    NOTE: This module overrides some of the functions decribed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_mapping(filename, content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given XML filename and XML file, extract rid mappings and attribute data\n",
    "    \"\"\"\n",
    "    \n",
    "    mappings  = {}\n",
    "    extracted = {}\n",
    "    references = {}\n",
    "    parsed    = xmltodict.parse(content.decode('UTF-8'))\n",
    "    soup      = BeautifulSoup(content)\n",
    "\n",
    "    if '@id' in parsed['article']:\n",
    "        extracted['id']       = str(parsed['article']['@id'])\n",
    "    if '@language' in parsed['article']:\n",
    "        extracted['language'] = str(parsed['article']['@language'])\n",
    "    if '@publish' in parsed['article']:\n",
    "        extracted['publish']  = str(parsed['article']['@publish'])\n",
    "    if '@relation' in parsed['article']:\n",
    "        extracted['relation'] = str(parsed['article']['@relation'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('jtl' in parsed['article']['pubfm']):\n",
    "                extracted['jtl']   = str(parsed['article']['pubfm']['jtl'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('vol' in parsed['article']['pubfm']):\n",
    "                extracted['vol']   = str(parsed['article']['pubfm']['vol'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('issue' in parsed['article']['pubfm']):\n",
    "                extracted['issue'] = str(parsed['article']['pubfm']['issue'])\n",
    "\n",
    "    if ('article' in parsed):\n",
    "        if ('pubfm' in parsed['article']):\n",
    "            if ('vol' in parsed['article']['pubfm']):\n",
    "                extracted['doi']   = str(parsed['article']['pubfm']['doi'])\n",
    "    \n",
    "    if ('article' in parsed):\n",
    "        if ('fm' in parsed['article']):\n",
    "            if ('atl' in parsed['article']['fm']):\n",
    "                extracted['title']   = str(parsed['article']['fm']['atl'])\n",
    "     \n",
    "    \n",
    "    del parsed\n",
    "    \n",
    "    #print(len(soup.find_all(\"bdy\")))\n",
    "    \n",
    "    text = ''\n",
    "    \n",
    "    for bib in soup.find_all(\"bdy\"):\n",
    "        \n",
    "        #print(bib.text)\n",
    "        \n",
    "        text += bib.text\n",
    "            \n",
    "    \n",
    "    references = {'metadata': papers[extracted['doi']], 'text': text}\n",
    "    \n",
    "    try:\n",
    "        with open('data/intermediate/full_text_mappings/%s' % (filename + '.json'), 'w') as f:\n",
    "            f.write(json.dumps(references))\n",
    "        return True\n",
    "    \n",
    "    except TypeError as e:\n",
    "        #print('Some contents of the file %s is not serializable' % filename)\n",
    "        raise e\n",
    "        \n",
    "        \n",
    "def get_zips_parallel_reducer(arg):\n",
    "    \n",
    "    try:\n",
    "        finfo, content = arg[0], zlib.decompress(arg[1])\n",
    "        del arg\n",
    "\n",
    "        get_text_mapping(finfo.filename, content)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print('Could not extract references from %s' % finfo)\n",
    "        return str(Exception)\n",
    "\n",
    "    \n",
    "def get_zips_parallel_mapper(directory):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to extract relevant files from the filebase\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        zip_list  = [i for i in os.listdir(directory) if 'supp_xml' not in i]\n",
    "        errored   = []\n",
    "        finfos    = []\n",
    "        contents  = []\n",
    "\n",
    "        for zfile in tqdm(zip_list):\n",
    "\n",
    "            zfile     = zipfile.ZipFile(os.path.join(directory, zfile))\n",
    "\n",
    "            for finfo in zfile.infolist():\n",
    "\n",
    "                if 'nature' in finfo.filename:\n",
    "                    \n",
    "                    ifile = zfile.open(finfo)\n",
    "                    content = ifile.read()\n",
    "\n",
    "                    finfos.append(finfo)\n",
    "                    contents.append(zlib.compress(content))\n",
    "\n",
    "        return finfos, contents\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print('Could not read file from zip %s, file %s' % (zfile, finfo.name))\n",
    "\n",
    "\n",
    "def onur_json(tex_ref_mappings, references):\n",
    "    \n",
    "    references_w_text = references.copy()\n",
    "    \n",
    "    for i in tqdm(range(len(references_w_text))):\n",
    "        \n",
    "        paper = tex_ref_mappings[i]\n",
    "\n",
    "        #print (paper)\n",
    "        for tex_ref_mapping in paper:\n",
    "\n",
    "            #print(tex_ref_mapping.values())\n",
    "\n",
    "            for ref_id in list(tex_ref_mapping.values())[0]:\n",
    "\n",
    "                if 'references' in references_w_text[i]:\n",
    "                    \n",
    "                    if ref_id in references_w_text[i]['references']:\n",
    "\n",
    "                        references_w_text[i]['references'][ref_id].update({'text' :list(tex_ref_mapping.keys())[0]})\n",
    "                        \n",
    "                    else:\n",
    "                        references_w_text[i]['references'][ref_id] = {'text' :list(tex_ref_mapping.keys())[0]}\n",
    "\n",
    "    return references\n",
    "\n",
    "def get_zips_parallel_reducer(arg):\n",
    "    \n",
    "    try:\n",
    "        finfo, content = arg[0], zlib.decompress(arg[1])\n",
    "        del arg\n",
    "        \n",
    "        get_reference_text(finfo.filename, content)\n",
    "        get_reference_mapping(finfo.filename, content)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        #print('Could not extract references from %s' % finfo)\n",
    "        return str(Exception)\n",
    "    \n",
    "    \n",
    "def flatten(l):\n",
    "    flat_list = []\n",
    "    for sublist in l:\n",
    "        if type(sublist) == list:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def combine_jsons(directory = 'data/intermediate/full_text_mappings',\n",
    "                  target = 'data/intermediate/matched_full_text.jsons'):\n",
    "\n",
    "    \n",
    "    with open (target, 'w') as f_to:\n",
    "        for file in tqdm(os.listdir(directory)):\n",
    "            with open(os.path.join(directory, file), 'r') as f_from:\n",
    "                f_to.write(f_from.read() + '\\n')\n",
    "                \n",
    "            \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nature_references_info['DOI'] = nature_references_info.index\n",
    "nature_references_info.head()\n",
    "\n",
    "papers = {k: g.to_dict(orient='records') for k, g in matched_dfs.groupby(level=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference in references: \n",
    "    \n",
    "    if 'doi' in reference['metadata']:\n",
    "        \n",
    "        if reference['metadata']['doi'] in papers:\n",
    "            \n",
    "            papers[reference['metadata']['doi']] = {'metadata' : reference['metadata'],\n",
    "                                                'references': papers[reference['metadata']['doi']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mp_input = zip(references, tex_ref_mappings)\n",
    "p        = Pool(8)\n",
    "\n",
    "matched_dfs = p.map(get_reference_map_onur, mp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/intermediate/matched.jsons', 'w') as f:\n",
    "\n",
    "        for index, paper in papers.items():\n",
    "\n",
    "            f.write('\\n')\n",
    "            f.write(json.dumps({index:paper}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "finfos, contents = get_zips_parallel_mapper(directory)\n",
    "p = Pool(7)\n",
    "success = p.map(get_zips_parallel_reducer, zip(finfos, contents))\n",
    "error_rate = sum([1 if i == True else 0 for i in success])/len(success)\n",
    "print ('%f of all processed files Succeeded' % error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_jsons()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
